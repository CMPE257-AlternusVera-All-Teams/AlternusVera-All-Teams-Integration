# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXoUf5SoN8bwCupGwsoprMCZ0Njxn_E3
"""

import pandas as pd
import numpy as np
import re
import pickle
from sklearn import feature_extraction
import xgboost as xgb
import statistics
from gensim.models.doc2vec import TaggedDocument
import nltk
nltk.download("stopwords") 

from nltk.corpus import stopwords
from nltk.corpus import words
from textblob import TextBlob, Word

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')



class StanceDitectionFeature():

    _wnl = nltk.WordNetLemmatizer()

    def gen_or_load_feats(self,feat_fn, headlines, bodies, feature_file):
      feats = feat_fn(headlines, bodies)
      np.save(feature_file, feats)
      return np.load(feature_file)

    def normalize_word(self,w):
        return self._wnl.lemmatize(w).lower()

    def get_tokenized_lemmas(self,s):
        return [self.normalize_word(t) for t in nltk.word_tokenize(s)]

    def clean(self,s):
        # Cleans a string: Lowercasing, trimming, removing non-alphanumeric
        return " ".join(re.findall(r'\w+', s, flags=re.UNICODE)).lower()

    def remove_stopwords(self,l):
        # Removes stopwords from a list of tokens
        return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]

    def word_overlap_features(self,headlines, bodies):
      X = []
      for i, (headline, body) in (enumerate(zip(headlines, bodies))):
          clean_headline = self.clean(headline)
          clean_body = self.clean(body)
          clean_headline = self.get_tokenized_lemmas(clean_headline)
          clean_body = self.get_tokenized_lemmas(clean_body)
          features = [
              len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]
          X.append(features)
          i = i+1
      return X

    def refuting_features(self,headlines, bodies):
      _refuting_words = [
          'fake',
          'fraud',
          'hoax',
          'false',
          'deny', 'denies',
          'not',
          'despite',
          'nope',
          'doubt', 'doubts',
          'bogus',
          'debunk',
          'pranks',
          'retract'
      ]
      X = []
      for i, (headline, body) in (enumerate(zip(headlines, bodies))):
          clean_headline = self.clean(headline)
          clean_headline = self.get_tokenized_lemmas(clean_headline)
          features = [1 if word in clean_headline else 0 for word in _refuting_words]
          X.append(features)
      return X

    def polarity_features(self,headlines, bodies):
      _refuting_words = [
          'fake',
          'fraud',
          'hoax',
          'false',
          'deny', 'denies',
          'not',
          'despite',
          'nope',
          'doubt', 'doubts',
          'bogus',
          'debunk',
          'pranks',
          'retract'
      ]

      def calculate_polarity(text):
          tokens = self.get_tokenized_lemmas(text)
          return sum([t in _refuting_words for t in tokens]) % 2
      X = []
      for i, (headline, body) in (enumerate(zip(headlines, bodies))):
          clean_headline = self.clean(headline)
          clean_body = self.clean(body)
          features = []
          features.append(calculate_polarity(clean_headline))
          features.append(calculate_polarity(clean_body))
          X.append(features)
      return np.array(X)

    def ngrams(self,input, n):
      input = input.split(' ')
      output = []
      for i in range(len(input) - n + 1):
          output.append(input[i:i + n])
      return output

    def chargrams(self,input, n):
        output = []
        for i in range(len(input) - n + 1):
            output.append(input[i:i + n])
        return output

    def append_chargrams(self,features, text_headline, text_body, size):
      grams = [' '.join(x) for x in self.chargrams(" ".join(self.remove_stopwords(text_headline.split())), size)]
      grams_hits = 0
      grams_early_hits = 0
      grams_first_hits = 0
      for gram in grams:
          if gram in text_body:
              grams_hits += 1
          if gram in text_body[:255]:
              grams_early_hits += 1
          if gram in text_body[:100]:
              grams_first_hits += 1
      features.append(grams_hits)
      features.append(grams_early_hits)
      features.append(grams_first_hits)
      return features

    def append_ngrams(self,features, text_headline, text_body, size):
        grams = [' '.join(x) for x in self.ngrams(text_headline, size)]
        grams_hits = 0
        grams_early_hits = 0
        for gram in grams:
            if gram in text_body:
                grams_hits += 1
            if gram in text_body[:255]:
                grams_early_hits += 1
        features.append(grams_hits)
        features.append(grams_early_hits)
        return features

    def hand_features(self,headlines, bodies):

      def binary_co_occurence(headline, body):
          # Count how many times a token in the title
          # appears in the body text.
          bin_count = 0
          bin_count_early = 0
          for headline_token in self.clean(headline).split(" "):
              if headline_token in self.clean(body):
                  bin_count += 1
              if headline_token in self.clean(body)[:255]:
                  bin_count_early += 1
          return [bin_count, bin_count_early]

      def binary_co_occurence_stops(headline, body):
          # Count how many times a token in the title
          # appears in the body text. Stopwords in the title
          # are ignored.
          bin_count = 0
          bin_count_early = 0
          for headline_token in self.remove_stopwords(self.clean(headline).split(" ")):
              if headline_token in self.clean(body):
                  bin_count += 1
                  bin_count_early += 1
          return [bin_count, bin_count_early]

      def count_grams(headline, body):
          # Count how many times an n-gram of the title
          # appears in the entire body, and intro paragraph

          clean_body = self.clean(body)
          clean_headline = self.clean(headline)
          features = []
          features = self.append_chargrams(features, clean_headline, clean_body, 2)
          features = self.append_chargrams(features, clean_headline, clean_body, 8)
          features = self.append_chargrams(features, clean_headline, clean_body, 4)
          features = self.append_chargrams(features, clean_headline, clean_body, 16)
          features = self.append_ngrams(features, clean_headline, clean_body, 2)
          features = self.append_ngrams(features, clean_headline, clean_body, 3)
          features = self.append_ngrams(features, clean_headline, clean_body, 4)
          features = self.append_ngrams(features, clean_headline, clean_body, 5)
          features = self.append_ngrams(features, clean_headline, clean_body, 6)
          return features

      X = []
      for i, (headline, body) in (enumerate(zip(headlines, bodies))):
          X.append(binary_co_occurence(headline, body)
                   + binary_co_occurence_stops(headline, body)
                   + count_grams(headline, body))

      return X

    ###
    LABELS = ['agree', 'disagree', 'discuss', 'unrelated']
    stance_score = {'agree': 1.0, 'disagree': 0.3,
                    'discuss': 0.8, 'unrelated': 0.0}

    def __generate_fn_features(self, dataset, name):
        h, b = [], []
 
        for d in dataset:
            h.append(d[0])  # title
            b.append(d[1])  # text

        X_overlap = self.gen_or_load_feats(
            self.word_overlap_features, h, b, self.base_path+"features/overlap."+name+".npy")
        X_refuting = self.gen_or_load_feats(
            self.refuting_features, h, b, self.base_path+"features/refuting."+name+".npy")
        X_polarity = self.gen_or_load_feats(
            self.polarity_features, h, b, self.base_path+"features/polarity."+name+".npy")
        X_hand = self.gen_or_load_feats(
            self.hand_features, h, b, self.base_path+"features/hand."+name+".npy")

        X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]
        return X

    def __predict_stance(self, headline, body):
        fn_predicted = ''

        # creating the dataframe with our text so we can leverage the existing code
        fn_dataset = pd.DataFrame(index=[0], columns=['Statement', 'text'])
        fn_dataset['Statement'] = headline
        fn_dataset['text'] = body
        fn_dataset = fn_dataset.values

        X_fn = self.__generate_fn_features(fn_dataset, "StanceFeature")

        self.fn_predicted = [self.LABELS[int(a)]
                             for a in self.bestModel.predict(X_fn)]

        return self.stance_score[self.fn_predicted[0]]

    def __init__(self, filenameModel, filenameBestModel, base_path):
        self.model = self.__load(filenameModel)
        self.bestModel = self.__load(filenameBestModel)
        self.base_path = base_path

    def __load(self, path):
        with open(path, 'rb') as file:
            return pickle.load(file)

    def FeatureFinders_getStanceScore(self, headline, body):
        x = self.__predict_stance(headline, body)
        xTrain = np.array(x).reshape(-1, 1)

        xPpredicted = self.model.predict(xTrain)
        xPredicedProb = self.model.predict_proba(xTrain)[:, 1]

        return float(xPredicedProb)


class ReliableSource():

    def __init__(self, filenameModel, filenameData):
        # path = "/content/drive/My Drive/MLFall2020/the-feature-finders/AlternusVera/ReliableSource/data.csv"
        self.model = self.__load(filenameModel)
        self.data = filenameData

    def __load(self, path):
        with open(path, 'rb') as file:
            return pickle.load(file)

    # return between 0 and 1, being 0 = True,  1 = Fake
    def __FeatureFinders_getSourceReliabilityScore(self, source):
        #			path = "/content/drive/My Drive/MLFall2020/the-feature-finders/AlternusVera/ReliableSource/data.csv"
        #			fakeNewsSites = pd.read_csv(path)
        fakeNewsSites = pd.read_csv(self.data)
        for index, row in fakeNewsSites.iterrows():
            score = 100
            if (row['Type of site'] == 'Some fake stories'):
                score = 50
            fakeNewsSites.at[index, 'fake_score'] = score

        if (source == ""):
            return 0
        d = fakeNewsSites[fakeNewsSites['Site name'].str.match(
            r'\b' + source + r'\b')]
        if d.shape[0] > 0:
            return d.iloc[0]['fake_score']

        return 0

    def FeatureFinders_getReliabilityBySource(self, src):
        x = self.__FeatureFinders_getSourceReliabilityScore(src)
        xTrain = np.array(x).reshape(-1, 1)

        xPpredicted = self.model.predict(xTrain)
        xPredicedProb = self.model.predict_proba(xTrain)[:, 1]

        return 1 - float(xPredicedProb)


class ToxicityFeature():

    def __init__(self, filenameModel, filenameBestModel, filenameFakeNewsmodel):
        self.model = self.__load(filenameModel)
        self.bestModel = self.__load(filenameBestModel)
        self.fakeNewsmodel = self.__load(filenameFakeNewsmodel)

    def __load(self, path):
        with open(path, 'rb') as file:
            return pickle.load(file)

    # Corpus cleaning
    def __clean_str(self, string):
        STOPWORDS = set(stopwords.words('english'))
        """
      Tokenization/string cleaning for datasets.
      Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
      """
        string = re.sub(r"^b", "", string)
        string = re.sub(r"\\n ", "", string)
        string = re.sub(r"\'s", "", string)
        string = re.sub(r"\'ve", "", string)
        string = re.sub(r"n\'t", "", string)
        string = re.sub(r"\'re", "", string)
        string = re.sub(r"\'d", "", string)
        string = re.sub(r"\'ll", "", string)
        string = re.sub(r",", "", string)
        string = re.sub(r"!", " ! ", string)
        string = re.sub(r"\(", "", string)
        string = re.sub(r"\)", "", string)
        string = re.sub(r"\?", "", string)
        string = re.sub(r"'", "", string)
        string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
        string = re.sub(r"[0-9]\w+|[0-9]", "", string)
        string = re.sub(r"\s{2,}", " ", string)
        string = ' '.join(Word(word).lemmatize() for word in string.split(
        ) if word not in STOPWORDS)  # delete stopwors from text
        return string.strip().lower()

    def __label_sentences(self, corpus, label_type):
        """
        Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.
        We do this by using the TaggedDocument method. The format will be "TRAIN_i" or "TEST_i" where "i" is
        a dummy index of the post.
        """
        labeled = []
        for i, v in enumerate(corpus):
            label = label_type + '_' + str(i)
            labeled.append(TaggedDocument(v.split(), [label]))
        return labeled

    def __get_vectors(self, model, corpus_size, vectors_size, vectors_type):
        """
        Get vectors from trained doc2vec model
        :param doc2vec_model: Trained Doc2Vec model
        :param corpus_size: Size of the data
        :param vectors_size: Size of the embedding vectors
        :param vectors_type: Training or Testing vectors
        :return: list of vectors
        """
        vectors = np.zeros((corpus_size, vectors_size))
        for i in range(0, corpus_size):
            prefix = vectors_type + '_' + str(i)
            vectors[i] = model.docvecs[prefix]
        return vectors

    def FeatureFinders_getToxicityScore(self, headline, body):
        # clean
        headline = self.__clean_str(headline)
        body = self.__clean_str(body)

        all_text = headline+body


        all_text = self.__label_sentences(all_text, "Test")

        # Doc2Vec
        test1_vectors_dbow = self.__get_vectors(
            self.model, len(all_text), 300, 'Test')

        # best Model for toxicity prediction
        # predictions
        predictedToxicity = self.bestModel.predict(test1_vectors_dbow)
        predictedToxicity = set(predictedToxicity)
        predictedToxicity = statistics.mean(predictedToxicity)

        predictedFakeNews = self.fakeNewsmodel.predict(predictedToxicity)
        predicedProb = self.fakeNewsmodel.predict_proba(predictedToxicity)[
            :, 0]

        return float(predicedProb[0])





# ----------------- helpers -------------------- #
english_stemmer = nltk.stem.SnowballStemmer('english')
token_pattern = r"(?u)\b\w\w+\b"
stopwords2 = set(nltk.corpus.stopwords.words('english'))


def stem_tokens(tokens, stemmer):
    stemmed = []
    for token in tokens:
        stemmed.append(stemmer.stem(token))
    return stemmed

def preprocess_data(line,
                    token_pattern=token_pattern,
                    exclude_stopword=True,
                    stem=True):
    # token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)
    token_pattern = re.compile(token_pattern, flags = re.UNICODE)
    tokens = [x.lower() for x in token_pattern.findall(line)]
    tokens_stemmed = tokens
    if stem:
        tokens_stemmed = stem_tokens(tokens, english_stemmer)
    if exclude_stopword:
        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords2]

    return tokens_stemmed

def try_divide(x, y, val=0.0):
    """ 
        Try to divide two numbers
    """
    if y != 0.0:
        val = float(x) / y
    return val


def cosine_sim(x, y):
    try:
        if type(x) is np.ndarray: x = x.reshape(1, -1) # get rid of the warning
        if type(y) is np.ndarray: y = y.reshape(1, -1)
        d = cosine_similarity(x, y)
        d = d[0][0]
    except:
        d = 0.
    return d

# ----------------- ngrams ------------------ #
def getUnigram(words):
    """
        Input: a list of words, e.g., ['I', 'am', 'Denny']
        Output: a list of unigram
    """
    assert type(words) == list
    return words
    
def getBigram(words, join_string, skip=0):
  """
     Input: a list of words, e.g., ['I', 'am', 'Denny']
     Output: a list of bigram, e.g., ['I_am', 'am_Denny']
     I use _ as join_string for this example.
  """
  assert type(words) == list
  L = len(words)
  if L > 1:
    lst = []
    for i in range(L-1):
      for k in range(1,skip+2):
        if i+k < L:
          lst.append( join_string.join([words[i], words[i+k]]) )
  else:
    # set it as unigram
    lst = getUnigram(words)
  return lst
    
def getTrigram(words, join_string, skip=0):
  """
     Input: a list of words, e.g., ['I', 'am', 'Denny']
     Output: a list of trigram, e.g., ['I_am_Denny']
     I use _ as join_string for this example.
  """
  assert type(words) == list
  L = len(words)
  if L > 2:
    lst = []
    for i in range(L-2):
      for k1 in range(1,skip+2):
        for k2 in range(1,skip+2):
          if i+k1 < L and i+k1+k2 < L:
            lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )
  else:
    # set it as bigram
    lst = getBigram(words, join_string, skip)
  return lst

# ----------------- score ------------------ #
LABELS = ['agree', 'disagree', 'discuss', 'unrelated']
LABELS_RELATED = ['unrelated','related']
RELATED = LABELS[0:3]

def score_submission(gold_labels, test_labels):
    score = 0.0
    cm = [[0, 0, 0, 0],
          [0, 0, 0, 0],
          [0, 0, 0, 0],
          [0, 0, 0, 0]]

    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):
        g_stance, t_stance = g, t
        if g_stance == t_stance:
            score += 0.25
            if g_stance != 'unrelated':
                score += 0.50
        if g_stance in RELATED and t_stance in RELATED:
            score += 0.25

        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1

    return score, cm


def print_confusion_matrix(cm):
    lines = []
    header = "|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|".format('', *LABELS)
    line_len = len(header)
    lines.append("-"*line_len)
    lines.append(header)
    lines.append("-"*line_len)

    hit = 0
    total = 0
    for i, row in enumerate(cm):
        hit += row[i]
        total += sum(row)
        lines.append("|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|".format(LABELS[i],
                                                                   *row))
        lines.append("-"*line_len)


def report_score(actual,predicted):
    score,cm = score_submission(actual,predicted)
    best_score, _ = score_submission(actual,actual)

    print_confusion_matrix(cm)
    return score*100/best_score

# ----------------- FeatureGenerator ------------------ #
class FeatureGenerator(object):
    def __init__(self, name):
        self._name = name
    
    def name(self):
        return self._name

    def process(self, data, header):
        '''
            input:
                data: pandas dataframe
            generate features and save them into a pickle file
        '''
        pass

    def read(self, header):
        '''
            read the feature matrix from a pickle file
        '''
        pass

# ----------------- CountFeatureGenerator ------------------ #
# from FeatureGenerator import *
from nltk.tokenize import sent_tokenize
# from helpers import *
import hashlib

class CountFeatureGenerator(FeatureGenerator):
    def __init__(self, name='countFeatureGenerator'):
        super(CountFeatureGenerator, self).__init__(name)


    def process(self, df, save_file=True, test_only=False):

        grams = ["unigram", "bigram", "trigram"]
        feat_names = ["Headline", "articleBody"]
        for feat_name in feat_names:
            for gram in grams:
                df["count_of_%s_%s" % (feat_name, gram)] = list(df.apply(lambda x: len(x[feat_name + "_" + gram]), axis=1))
                df["count_of_unique_%s_%s" % (feat_name, gram)] = \
		            list(df.apply(lambda x: len(set(x[feat_name + "_" + gram])), axis=1))
                df["ratio_of_unique_%s_%s" % (feat_name, gram)] = \
                    list(map(try_divide, df["count_of_unique_%s_%s"%(feat_name,gram)], df["count_of_%s_%s"%(feat_name,gram)]))

        # overlapping n-grams count
        for gram in grams:
            df["count_of_Headline_%s_in_articleBody" % gram] = \
                list(df.apply(lambda x: sum([1. for w in x["Headline_" + gram] if w in set(x["articleBody_" + gram])]), axis=1))
            df["ratio_of_Headline_%s_in_articleBody" % gram] = \
                list(map(try_divide, df["count_of_Headline_%s_in_articleBody" % gram], df["count_of_Headline_%s" % gram]))
        
        # number of sentences in headline and body
        for feat_name in feat_names:
            #df['len_sent_%s' % feat_name] = df[feat_name].apply(lambda x: len(sent_tokenize(x.decode('utf-8').encode('ascii', errors='ignore'))))
            df['len_sent_%s' % feat_name] = df[feat_name].apply(lambda x: len(sent_tokenize(x)))

        # dump the basic counting features into a file
        feat_names = [ n for n in df.columns \
                if "count" in n \
                or "ratio" in n \
                or "len_sent" in n]
        
        # binary refuting features
        _refuting_words = [
            'fake',
            'fraud',
            'hoax',
            'false',
            'deny', 'denies',
            # 'refute',
            'not',
            'despite',
            'nope',
            'doubt', 'doubts',
            'bogus',
            'debunk',
            'pranks',
            'retract'
        ]

        _hedging_seed_words = [
            'alleged', 'allegedly',
            'apparently',
            'appear', 'appears',
            'claim', 'claims',
            'could',
            'evidently',
            'largely',
            'likely',
            'mainly',
            'may', 'maybe', 'might',
            'mostly',
            'perhaps',
            'presumably',
            'probably',
            'purported', 'purportedly',
            'reported', 'reportedly',
            'rumor', 'rumour', 'rumors', 'rumours', 'rumored', 'rumoured',
            'says',
            'seem',
            'somewhat',
            # 'supposedly',
            'unconfirmed'
        ]
        
        check_words = _refuting_words
        for rf in check_words:
            fname = '%s_exist' % rf
            feat_names.append(fname)
            df[fname] = list(df['Headline'].map(lambda x: 1 if rf in x else 0))
	    
        if test_only:
            xBasicCountsTrain = df[feat_names].values
            return [xBasicCountsTrain]
            
        
        # split into train, test portion and save in separate files
        train = df[~df['target'].isnull()]
        xBasicCountsTrain = train[feat_names].values
        outfilename_bcf_train = "train.basic.pkl"
        if save_file:
            with open(outfilename_bcf_train, "wb") as outfile:
                pickle.dump(feat_names, outfile, -1)
                pickle.dump(xBasicCountsTrain, outfile, -1)

        
        test = df[df['target'].isnull()]
        #return 1
        if test.shape[0] > 0:
            # test set exists
            if save_file:
                xBasicCountsTest = test[feat_names].values
                outfilename_bcf_test = "test.basic.pkl"
                with open(outfilename_bcf_test, 'wb') as outfile:
                    pickle.dump(feat_names, outfile, -1)
                    pickle.dump(xBasicCountsTest, outfile, -1)

        return [xBasicCountsTest]


    def read(self, header='train'):

        filename_bcf = "%s.basic.pkl" % header
        with open(fakenews_path + filename_bcf, "rb") as infile:
            feat_names = pickle.load(infile)
            xBasicCounts = pickle.load(infile)
            np.save('counts_test', [xBasicCounts])

        return [xBasicCounts]


# ----------------- TfidfFeatureGenerator ------------------ #
# from FeatureGenerator import *
from sklearn.feature_extraction.text import TfidfVectorizer

class TfidfFeatureGenerator(FeatureGenerator):
    def __init__(self, name='tfidfFeatureGenerator'):
        super(TfidfFeatureGenerator, self).__init__(name)
    
    def process(self, df, save_file=True, test_only=False):
        
       # 1). create strings based on ' '.join(Headline_unigram + articleBody_unigram) [ already stemmed ]
        def cat_text(x):
            res = '%s %s' % (' '.join(x['Headline_unigram']), ' '.join(x['articleBody_unigram']))
            return res
        df["all_text"] = list(df.apply(cat_text, axis=1))

        if test_only:
            n_train = df.shape[0]
            n_test = 0
        else:
            n_train = df[~df['target'].isnull()].shape[0]
            n_test = df[df['target'].isnull()].shape[0]

        # 2). fit a TfidfVectorizer on the concatenated strings
        # 3). sepatately transform ' '.join(Headline_unigram) and ' '.join(articleBody_unigram)
        vec = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2)
        if test_only:
            vec = TfidfVectorizer(ngram_range=(1, 3), max_df=1, min_df=0)
        vec.fit(df["all_text"]) # Tf-idf calculated on the combined training + test set
        vocabulary = vec.vocabulary_

        vecH = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)
        if test_only:
            vecH = TfidfVectorizer(ngram_range=(1, 3), max_df=1, min_df=0, vocabulary=vocabulary)
        xHeadlineTfidf = vecH.fit_transform(df['Headline_unigram'].map(lambda x: ' '.join(x))) # use ' '.join(Headline_unigram) instead of Headline since the former is already stemmed
        
        # save train and test into separate files
        xHeadlineTfidfTrain = xHeadlineTfidf[:n_train, :]
        if save_file:
            outfilename_htfidf_train = "train.headline.tfidf.pkl"
            with open(outfilename_htfidf_train, "wb") as outfile:
                pickle.dump(xHeadlineTfidfTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            xHeadlineTfidfTest = xHeadlineTfidf[n_train:, :]
            if save_file:
                outfilename_htfidf_test = "test.headline.tfidf.pkl"
                with open(outfilename_htfidf_test, "wb") as outfile:
                    pickle.dump(xHeadlineTfidfTest, outfile, -1)


        vecB = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)
        xBodyTfidf = vecB.fit_transform(df['articleBody_unigram'].map(lambda x: ' '.join(x)))
        
        # save train and test into separate files
        xBodyTfidfTrain = xBodyTfidf[:n_train, :]
        if save_file:
            outfilename_btfidf_train = "train.body.tfidf.pkl"
            with open(outfilename_btfidf_train, "wb") as outfile:
                pickle.dump(xBodyTfidfTrain, outfile, -1)
        
        if n_test > 0:
            # test set is availble
            xBodyTfidfTest = xBodyTfidf[n_train:, :]
            if save_file:
                outfilename_btfidf_test = "test.body.tfidf.pkl"
                with open(outfilename_btfidf_test, "wb") as outfile:
                    pickle.dump(xBodyTfidfTest, outfile, -1)
               

        # 4). compute cosine similarity between headline tfidf features and body tfidf features
        # simTfidf = np.asarray(map(cosine_sim, xHeadlineTfidf, xBodyTfidf))[:, np.newaxis]
        # work-around for array indice error
        res = []
        if test_only:
          for i in range(0, 1):
              res.append(cosine_sim(xHeadlineTfidf[i], xBodyTfidf[i]))
        else:
          for i in range(0, 75385):
            res.append(cosine_sim(xHeadlineTfidf[i], xBodyTfidf[i]))
            
        simTfidf = np.asarray(list(res))[:, np.newaxis]
        #

        simTfidfTrain = simTfidf[:n_train]
        if save_file:
            outfilename_simtfidf_train = "train.sim.tfidf.pkl"
            with open(outfilename_simtfidf_train, "wb") as outfile:
                pickle.dump(simTfidfTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            simTfidfTest = simTfidf[n_train:]
            if save_file:
                outfilename_simtfidf_test = "test.sim.tfidf.pkl"
                with open(outfilename_simtfidf_test, "wb") as outfile:
                    pickle.dump(simTfidfTest, outfile, -1)

        return [xHeadlineTfidf, xBodyTfidf, simTfidf]
        # return [simTfidf.reshape(-1, 1)]

    def read(self, header = 'train'):
        filename_htfidf = "%s.headline.tfidf.pkl" % header
        with open(fakenews_path + filename_htfidf, "rb") as infile:
            xHeadlineTfidf = pickle.load(infile)

        filename_btfidf = "%s.body.tfidf.pkl" % header
        with open(fakenews_path + filename_btfidf, "rb") as infile:
            xBodyTfidf = pickle.load(infile)

        filename_simtfidf = "%s.sim.tfidf.pkl" % header
        with open(fakenews_path + filename_simtfidf, "rb") as infile:
            simTfidf = pickle.load(infile)


        # return [xHeadlineTfidf, xBodyTfidf, simTfidf.reshape(-1, 1)]
        return [simTfidf.reshape(-1, 1)]

# ----------------- SvdFeatureGenerator ------------------ #
from scipy.sparse import vstack
from sklearn.decomposition import TruncatedSVD

class SvdFeatureGenerator(FeatureGenerator):
    def __init__(self, name='svdFeatureGenerator'):
        super(SvdFeatureGenerator, self).__init__(name)


    def process(self, df, xHeadlineTfidfTrain=None, xBodyTfidfTrain=None, save_file=True, test_only=False):
        
        if test_only:
            n_train = df.shape[0]
            n_test = 0
        else:
            n_train = df[~df['target'].isnull()].shape[0]
            n_test  = df[df['target'].isnull()].shape[0]


        if xHeadlineTfidfTrain is not None and xBodyTfidfTrain is not None:
            pass 
        else:
            tfidfGenerator = TfidfFeatureGenerator('tfidf')
            featuresTrain = tfidfGenerator.read('train')
            xHeadlineTfidfTrain, xBodyTfidfTrain = featuresTrain[0], featuresTrain[1]
        
        xHeadlineTfidf = xHeadlineTfidfTrain
        xBodyTfidf = xBodyTfidfTrain
        if n_test > 0:
            # test set is available
            featuresTest  = tfidfGenerator.read('test')
            xHeadlineTfidfTest,  xBodyTfidfTest  = featuresTest[0],  featuresTest[1]
            xHeadlineTfidf = vstack([xHeadlineTfidfTrain, xHeadlineTfidfTest])
            xBodyTfidf = vstack([xBodyTfidfTrain, xBodyTfidfTest])
	    
        # compute the cosine similarity between truncated-svd features
        # svd = TruncatedSVD(n_components=50, n_iter=15)
        svd = TruncatedSVD(n_components=15, n_iter=15)
        xHBTfidf = vstack([xHeadlineTfidf, xBodyTfidf])
        svd.fit(xHBTfidf) # fit to the combined train-test set (or the full training set for cv process)
        xHeadlineSvd = svd.transform(xHeadlineTfidf)
        
        xHeadlineSvdTrain = xHeadlineSvd[:n_train, :]
        if save_file:
            outfilename_hsvd_train = "train.headline.svd.pkl"
            with open(outfilename_hsvd_train, "wb") as outfile:
                pickle.dump(xHeadlineSvdTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            xHeadlineSvdTest = xHeadlineSvd[n_train:, :]
            if save_file:
                outfilename_hsvd_test = "test.headline.svd.pkl"
                with open(outfilename_hsvd_test, "wb") as outfile:
                    pickle.dump(xHeadlineSvdTest, outfile, -1)

        xBodySvd = svd.transform(xBodyTfidf)
        
        xBodySvdTrain = xBodySvd[:n_train, :]
        if save_file:
            outfilename_bsvd_train = "train.body.svd.pkl"
            with open(outfilename_bsvd_train, "wb") as outfile:
                pickle.dump(xBodySvdTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            xBodySvdTest = xBodySvd[n_train:, :]
            if save_file:
                outfilename_bsvd_test = "test.body.svd.pkl"
                with open(outfilename_bsvd_test, "wb") as outfile:
                    pickle.dump(xBodySvdTest, outfile, -1)

        # work-around for array indice error
        # simSvd = np.asarray(map(cosine_sim, xHeadlineSvd, xBodySvd))[:, np.newaxis]
        res = []
        if test_only:
          for i in range(0, 1):
            res.append(cosine_sim(xHeadlineSvd[i], xBodySvd[i]))
        else:
          for i in range(0, 75385):
            res.append(cosine_sim(xHeadlineSvd[i], xBodySvd[i]))

        simSvd = np.asarray(list(res))[:, np.newaxis]
        

        simSvdTrain = simSvd[:n_train]
        if save_file:
            outfilename_simsvd_train = "train.sim.svd.pkl"
            with open(outfilename_simsvd_train, "wb") as outfile:
                pickle.dump(simSvdTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            simSvdTest = simSvd[n_train:]
            outfilename_simsvd_test = "test.sim.svd.pkl"
            if save_file:
                with open(outfilename_simsvd_test, "wb") as outfile:
                    pickle.dump(simSvdTest, outfile, -1)

        if test_only:
            # pad with 0 since the model has much more values after truncated

            result1 = np.pad(xHeadlineSvd[0], (0, 48), 'constant') # pad with 0s, 0 offset left, 48 right
            xHeadlineSvd = [result1]
            # array([0, 0, 1, 2, 3, 4, 5, 0, 0, 0])
            result2 = np.pad(xBodySvd[0], (0, 48), 'constant') # pad with 0s, 0 offset left, 48 right
            xBodySvd = [result2]
            

            return [xHeadlineSvd, xBodySvd, simSvd.reshape(-1, 1)]


        return [xHeadlineSvd, xBodySvd, simSvd.reshape(-1, 1)]


    def read(self, header='train'):

        filename_hsvd = "%s.headline.svd.pkl" % header
        with open(fakenews_path + filename_hsvd, "rb") as infile:
            xHeadlineSvd = pickle.load(infile)

        filename_bsvd = "%s.body.svd.pkl" % header
        with open(fakenews_path + filename_bsvd, "rb") as infile:
            xBodySvd = pickle.load(infile)

        filename_simsvd = "%s.sim.svd.pkl" % header
        with open(fakenews_path + filename_simsvd, "rb") as infile:
            simSvd = pickle.load(infile)

        return [xHeadlineSvd, xBodySvd, simSvd.reshape(-1, 1)]
        #return [simSvd.reshape(-1, 1)]

# ----------------- Word2VecFeatureGenerator ------------------ #
# import gensim
from sklearn.preprocessing import normalize
from functools import reduce

class Word2VecFeatureGenerator(FeatureGenerator):

    def __init__(self, gensim, name='word2vecFeatureGenerator'):
        super(Word2VecFeatureGenerator, self).__init__(name)
        self.modelGensim = gensim

    def process(self, df, save_file=True, test_only=False):

        df["Headline_unigram_vec"] = df["Headline"].map(lambda x: preprocess_data(x, exclude_stopword=False, stem=False))
        df["articleBody_unigram_vec"] = df["articleBody"].map(lambda x: preprocess_data(x, exclude_stopword=False, stem=False))
        
        if test_only:
          n_train = df.shape[0]
          n_test = 0
        else:
          n_train = df[~df['target'].isnull()].shape[0]
          n_test = df[df['target'].isnull()].shape[0]
        
        # 1). document vector built by multiplying together all the word vectors
        # using Google's pre-trained word vectors
        # model = gensim.models.KeyedVectors.load_word2vec_format("/content/drive/My Drive/mydata/fakenewschallenge/GoogleNews-vectors-negative300.bin", binary=True)

        Headline_unigram_array = df['Headline_unigram_vec'].values
        
        # word vectors weighted by normalized tf-idf coefficient?
        #headlineVec = [0]

        headlineVec = list(map(lambda x: reduce(np.add, [self.modelGensim[y] for y in x if y in self.modelGensim], [0.]*300), Headline_unigram_array))
        headlineVec = np.array(headlineVec)
        #headlineVec = np.exp(headlineVec)
        headlineVec = normalize(headlineVec)
        
        headlineVecTrain = headlineVec[:n_train, :]
        if save_file:
            outfilename_hvec_train = "train.headline.word2vec.pkl"
            with open(outfilename_hvec_train, "wb") as outfile:
                pickle.dump(headlineVecTrain, outfile, -1)

        if n_test > 0:
            # test set is available
            headlineVecTest = headlineVec[n_train:, :]
            if save_file:
                outfilename_hvec_test = "test.headline.word2vec.pkl"
                with open(outfilename_hvec_test, "wb") as outfile:
                    pickle.dump(headlineVecTest, outfile, -1)

        Body_unigram_array = df['articleBody_unigram_vec'].values
        #bodyVec = [0]
        bodyVec = list(map(lambda x: reduce(np.add, [self.modelGensim[y] for y in x if y in self.modelGensim], [0.]*300), Body_unigram_array))
        bodyVec = np.array(bodyVec)
        bodyVec = normalize(bodyVec)

        bodyVecTrain = bodyVec[:n_train, :]
        if save_file:
            outfilename_bvec_train = "train.body.word2vec.pkl"
            with open(outfilename_bvec_train, "wb") as outfile:
                pickle.dump(bodyVecTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            bodyVecTest = bodyVec[n_train:, :]
            if save_file:
                outfilename_bvec_test = "test.body.word2vec.pkl"
                with open(outfilename_bvec_test, "wb") as outfile:
                    pickle.dump(bodyVecTest, outfile, -1)


        # compute cosine similarity between headline/body word2vec features
        # simVec = np.asarray(map(cosine_sim, headlineVec, bodyVec))[:, np.newaxis]
        # work-around for indice error
        res = []
        if test_only:
            for i in range(0, 1):
                res.append(cosine_sim(headlineVec[i], bodyVec[i]))
        else:
            for i in range(0, 75385):
                res.append(cosine_sim(headlineVec[i], bodyVec[i]))
        simVec = np.asarray(list(res))[:, np.newaxis]

        simVecTrain = simVec[:n_train]
        if save_file:
            outfilename_simvec_train = "train.sim.word2vec.pkl"
            with open(outfilename_simvec_train, "wb") as outfile:
                pickle.dump(simVecTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            simVecTest = simVec[n_train:]
            if save_file:
                outfilename_simvec_test = "test.sim.word2vec.pkl"
                with open(outfilename_simvec_test, "wb") as outfile:
                    pickle.dump(simVecTest, outfile, -1)

        return [headlineVecTrain, bodyVecTrain, simVecTrain]

    def read(self, header='train'):

        filename_hvec = "%s.headline.word2vec.pkl" % header
        with open(fakenews_path + filename_hvec, "rb") as infile:
            headlineVec = pickle.load(infile)

        filename_bvec = "%s.body.word2vec.pkl" % header
        with open(fakenews_path + filename_bvec, "rb") as infile:
            bodyVec = pickle.load(infile)

        filename_simvec = "%s.sim.word2vec.pkl" % header
        with open(fakenews_path + filename_simvec, "rb") as infile:
            simVec = pickle.load(infile)


        return [headlineVec, bodyVec, simVec]
        #return [simVec.reshape(-1,1)]

# ----------------- SentimentFeatureGenerator ------------------ #
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import sent_tokenize

class SentimentFeatureGenerator(FeatureGenerator):
    def __init__(self, name='sentimentFeatureGenerator'):
        super(SentimentFeatureGenerator, self).__init__(name)


    def process(self, df, save_file=True, test_only=False):

        
        if test_only:
          n_train = df.shape[0]
          n_test = 0
        else:
          n_train = df[~df['target'].isnull()].shape[0]
          n_test = df[df['target'].isnull()].shape[0]

        # calculate the polarity score of each sentence then take the average
        sid = SentimentIntensityAnalyzer()
        def compute_sentiment(sentences):
            result = []
            for sentence in sentences:
                vs = sid.polarity_scores(sentence)
                result.append(vs)
            return pd.DataFrame(result).mean()
        
        #df['headline_sents'] = df['Headline'].apply(lambda x: sent_tokenize(x.decode('utf-8')))
        df['headline_sents'] = df['Headline'].apply(lambda x: sent_tokenize(x))
        df = pd.concat([df, df['headline_sents'].apply(lambda x: compute_sentiment(x))], axis=1)
        df.rename(columns={'compound':'h_compound', 'neg':'h_neg', 'neu':'h_neu', 'pos':'h_pos'}, inplace=True)
        headlineSenti = df[['h_compound','h_neg','h_neu','h_pos']].values
        
        headlineSentiTrain = headlineSenti[:n_train, :]
        if save_file:
            outfilename_hsenti_train = "train.headline.senti.pkl"
            with open(outfilename_hsenti_train, "wb") as outfile:
                pickle.dump(headlineSentiTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            headlineSentiTest = headlineSenti[n_train:, :]
            if save_file:
                outfilename_hsenti_test = "test.headline.senti.pkl"
                with open(outfilename_hsenti_test, "wb") as outfile:
                    pickle.dump(headlineSentiTest, outfile, -1)
        
        #return 1

        df['body_sents'] = df['articleBody'].map(lambda x: sent_tokenize(x))
        df = pd.concat([df, df['body_sents'].apply(lambda x: compute_sentiment(x))], axis=1)
        df.rename(columns={'compound':'b_compound', 'neg':'b_neg', 'neu':'b_neu', 'pos':'b_pos'}, inplace=True)
        bodySenti = df[['b_compound','b_neg','b_neu','b_pos']].values
        
        bodySentiTrain = bodySenti[:n_train, :]
        if save_file:
            outfilename_bsenti_train = "train.body.senti.pkl"
            with open(outfilename_bsenti_train, "wb") as outfile:
                pickle.dump(bodySentiTrain, outfile, -1)
        
        if n_test > 0:
            # test set is available
            bodySentiTest = bodySenti[n_train:, :]
            if save_file:
                outfilename_bsenti_test = "test.body.senti.pkl"
                with open(outfilename_bsenti_test, "wb") as outfile:
                    pickle.dump(bodySentiTest, outfile, -1)

        return [headlineSentiTrain, bodySentiTrain]


    def read(self, header='train'):

        filename_hsenti = "%s.headline.senti.pkl" % header
        with open(fakenews_path + filename_hsenti, "rb") as infile:
            headlineSenti = pickle.load(infile)

        filename_bsenti = "%s.body.senti.pkl" % header
        with open(fakenews_path + filename_bsenti, "rb") as infile:
            bodySenti = pickle.load(infile)
        np.save('senti_headline_body_test', [headlineSenti, bodySenti])

        return [headlineSenti, bodySenti]



# ----------------- TitleVsBody ------------------ #

class TitleVsBody():

    def __init__(self, filenameModelLog, filenameModelXgb, gensimModel): #, filenameBestModel, base_path):
        self.modelLog = self.__load(filenameModelLog)
        self.modelXgb = self.__load(filenameModelXgb)
        self.modelGensim = gensimModel

    def __load(self, path):
        with open(path, 'rb') as file:
            return pickle.load(file)


    '''
    Return a list with a single predicted value (int 0-3) for Title Vs Body 
    '''
    def FeatureFinders_getTitleVsBodyRelationship(self, head_line="", body_text=""):
        head_line = head_line.strip()
        body_text = body_text.strip()

        if not head_line or not body_text: # return 3-unrelated if BAD data!!!
            # print("Bad body data")
            return [3]

        init_data = { 'Headline': [head_line], 'articleBody': [body_text] }

        data = pd.DataFrame(init_data, columns = ['Headline', 'articleBody'])

        # generate unigram
        data["Headline_unigram"] = data["Headline"].map(lambda x: preprocess_data(x))
        data["articleBody_unigram"] = data["articleBody"].map(lambda x: preprocess_data(x))

        # generate bigram
        join_str = "_"
        data["Headline_bigram"] = data["Headline_unigram"].map(lambda x: getBigram(x, join_str))
        data["articleBody_bigram"] = data["articleBody_unigram"].map(lambda x: getBigram(x, join_str))

        # generate trigram
        data["Headline_trigram"] = data["Headline_unigram"].map(lambda x: getTrigram(x, join_str))
        data["articleBody_trigram"] = data["articleBody_unigram"].map(lambda x: getTrigram(x, join_str))
        
        c_fg = CountFeatureGenerator()
        basic_count = c_fg.process(data, save_file=False, test_only=True)

        tfidf_fg = TfidfFeatureGenerator()
        # tfidf = [xHeadlineTfidf, xBodyTfidf, simTfidf]
        tfidf = tfidf_fg.process(data, save_file=False, test_only=True)
        
        # tfidfGenerator = TfidfFeatureGenerator('tfidf')
        # featuresTrain = tfidfGenerator.read('train')
        xHeadlineTfidfTrain, xBodyTfidfTrain, simTfidf = tfidf[0], tfidf[1], tfidf[2]
        simTfidf = [simTfidf.reshape(-1, 1)]
        # return [simTfidf.reshape(-1, 1)]


        # [xHeadlineSvd, xBodySvd, simSvd.reshape(-1, 1)]
        svd_fg = SvdFeatureGenerator()
        # svd = [xHeadlineSvd, xBodySvd, simSvd.reshape(-1, 1)]
        svd = svd_fg.process(data, xHeadlineTfidfTrain=xHeadlineTfidfTrain, xBodyTfidfTrain=xBodyTfidfTrain, save_file=False, test_only=True)


        w2v_fg = Word2VecFeatureGenerator(self.modelGensim, name='word2vecFeatureGenerator')
        # w2v = [headlineVecTrain, bodyVecTrain, simVecTrain]
        w2v = w2v_fg.process(data, save_file=False, test_only=True)

        senti_fg = SentimentFeatureGenerator()
        # senti = [headlineSentiTrain, bodySentiTrain]
        senti = senti_fg.process(data, save_file=False, test_only=True)
        
        # features = [f for g in generators for f in g.read('train')]
        values = [basic_count, simTfidf, svd, w2v, senti]
        features = [f for v in values for f in v]
        # for v in values:
        #     for f in v:
        #         features.append(f)
        
        data_x = np.hstack(features)
        test_x = data_x


        # dtrain = xgb.DMatrix(data_x, label=data_y, weight=w)
        dtest = xgb.DMatrix(test_x)

        pred = self.modelXgb.predict(dtest)
        pred_prob_y = pred.reshape(test_x.shape[0], 4)
        pred_prob_y = self.modelXgb.predict(dtest).reshape(test_x.shape[0], 4)

        pred_y = np.argmax(pred_prob_y, axis = 1)
        
        return pred_y

    def FeatureFinders_getTitleVsBodyScore(self, val): # int val 0-3 of TitleVsBody labels
        arr = np.array([val])
        return self.modelLog.predict_proba(arr.reshape(-1, 1))



